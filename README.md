# awesome-python-benchmarks
Benchmarking for python packages 


## Python time-series benchmarking


* [Time-Series Elo ratings](https://microprediction.github.io/timeseries-elo-ratings/html_leaderboards/overall.html) considers methods for autonomous univariate prediction of relatively short sequences (400 lags) and ranks performance on predictions from 1 to 34 steps ahead. 



## Python black-box derivative free benchmarking

* [Coco](https://github.com/numbbo/coco)

* [BBOB workshop series](http://numbbo.github.io/workshops/index.html) features ten workshops, most recently the [2019 workshop](http://numbbo.github.io/workshops/BBOB-2019/index.html) on black box methods. 

* [Optimizer Elo ratings](https://microprediction.github.io/optimizer-elo-ratings/html_leaderboards/overall.html) rates a hundred approaches to derivative free optimization on an ongoing basis, with methods taken from packages such as NLOPT, Nevergrad, BayesOpt, PySOT, Skopt, Bobyqa, Hebo, Optuna and many others. 



## R Time-series 

* [ForecastBenchmark](https://github.com/DescartesResearch/ForecastBenchmark) automatically evaluates and ranks forecasting methods based on their performance in a diverse set of evaluation scenarios. The benchmark comprises four different use cases, each covering 100 heterogeneous time series taken from different domains. 

